"""
Script de dÃ©tection des doublons dans les fichiers JSONL d'offres France Travail.

Ce script analyse les fichiers JSONL normalisÃ©s et identifie les offres en doublons
basÃ©es sur le champ "id". Affiche un rapport dÃ©taillÃ© sans modifier les fichiers.

Usage:
    python scripts/analysis/find_duplicates.py
"""

import json
from pathlib import Path
from typing import Dict, List, Set
from collections import defaultdict


def find_duplicates_in_file(file_path: Path) -> Dict:
    """
    Identifie les doublons dans un fichier JSONL.
    
    Args:
        file_path: Chemin du fichier Ã  analyser
    
    Returns:
        Dictionnaire avec les statistiques et dÃ©tails des doublons
    """
    seen_ids: Dict[str, List[int]] = defaultdict(list)
    all_offers: List[dict] = []
    total_count = 0
    
    # Lecture du fichier
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            
            try:
                record = json.loads(line)
                total_count += 1
                all_offers.append(record)
                
                record_id = record.get('id')
                if not record_id:
                    print(f"  âš ï¸  Ligne {line_num}: pas d'ID trouvÃ©")
                    continue
                
                # Enregistrer la ligne oÃ¹ cet ID apparaÃ®t
                seen_ids[record_id].append(line_num)
            
            except json.JSONDecodeError as e:
                print(f"  âš ï¸  Ligne {line_num}: erreur JSON - {e}")
                continue
    
    # Identifier les IDs en doublons (apparaissent plus d'une fois)
    duplicates = {
        offer_id: line_nums 
        for offer_id, line_nums in seen_ids.items() 
        if len(line_nums) > 1
    }
    
    # CrÃ©er un rapport dÃ©taillÃ© pour chaque doublon
    duplicate_details = []
    for offer_id, line_nums in duplicates.items():
        # Trouver les offres correspondantes
        matching_offers = [
            offer for offer in all_offers 
            if offer.get('id') == offer_id
        ]
        
        if matching_offers:
            detail = {
                'id': offer_id,
                'occurrences': len(line_nums),
                'line_numbers': line_nums,
                'title': matching_offers[0].get('title', 'N/A'),
                'company': matching_offers[0].get('company_name', 'N/A')
            }
            duplicate_details.append(detail)
    
    unique_count = len(seen_ids)
    duplicate_count = sum(len(line_nums) - 1 for line_nums in duplicates.values())
    
    return {
        'total': total_count,
        'unique': unique_count,
        'duplicates': duplicate_count,
        'duplicate_ids': len(duplicates),
        'details': duplicate_details
    }


def analyze_directory(directory: Path, show_details: bool = True) -> Dict:
    """
    Analyse tous les fichiers JSONL d'un dossier.
    
    Args:
        directory: Chemin du dossier Ã  analyser
        show_details: Afficher les dÃ©tails de chaque doublon
    
    Returns:
        Statistiques globales
    """
    if not directory.exists():
        print(f"âŒ Dossier non trouvÃ©: {directory}")
        return {}
    
    jsonl_files = [
        f for f in directory.glob("*.jsonl")
        if not f.stem.endswith('_deduplicate')  # Ignorer les fichiers dÃ©dupliquÃ©s
    ]
    
    if not jsonl_files:
        print(f"â„¹ï¸  Aucun fichier JSONL trouvÃ© dans {directory}")
        return {}
    
    print(f"\nðŸ“ Analyse du dossier: {directory.relative_to(Path.cwd())}")
    print(f"   Fichiers Ã  analyser: {len(jsonl_files)}\n")
    
    global_stats = {
        'total_files': len(jsonl_files),
        'total_offers': 0,
        'total_duplicates': 0,
        'files_with_duplicates': 0
    }
    
    for file_path in sorted(jsonl_files):
        print(f"  ðŸ” {file_path.name}")
        
        stats = find_duplicates_in_file(file_path)
        
        global_stats['total_offers'] += stats['total']
        global_stats['total_duplicates'] += stats['duplicates']
        
        print(f"     ðŸ“Š Total d'offres: {stats['total']}")
        print(f"     âœ… IDs uniques: {stats['unique']}")
        
        if stats['duplicates'] > 0:
            global_stats['files_with_duplicates'] += 1
            print(f"     âš ï¸  Doublons trouvÃ©s: {stats['duplicates']} (dans {stats['duplicate_ids']} IDs)")
            
            if show_details and stats['details']:
                print(f"\n     ðŸ“‹ DÃ©tails des doublons:")
                for detail in stats['details'][:5]:  # Limiter Ã  5 premiers pour lisibilitÃ©
                    print(f"        â€¢ ID: {detail['id']}")
                    print(f"          Titre: {detail['title'][:70]}...")
                    print(f"          Occurrences: {detail['occurrences']} (lignes {detail['line_numbers']})")
                
                if len(stats['details']) > 5:
                    print(f"        ... et {len(stats['details']) - 5} autres doublons")
                print()
        else:
            print(f"     âœ… Aucun doublon trouvÃ©")
        
        print()
    
    return global_stats


def main():
    """Point d'entrÃ©e principal du script."""
    print("=" * 80)
    print(" ðŸ” DÃ©tection des doublons dans les offres France Travail")
    print("=" * 80)
    
    # DÃ©finir le chemin du dossier normalisÃ©
    base_path = Path(__file__).parent.parent.parent
    normalized_dir = base_path / "data" / "normalized" / "francetravail"
    
    # Analyser le dossier
    stats = analyze_directory(normalized_dir, show_details=True)
    
    # Afficher le rÃ©sumÃ© global
    if stats:
        print("=" * 80)
        print(" ðŸ“Š RÃ‰SUMÃ‰ GLOBAL")
        print("=" * 80)
        print(f"  Fichiers analysÃ©s: {stats['total_files']}")
        print(f"  Offres totales: {stats['total_offers']}")
        print(f"  Doublons dÃ©tectÃ©s: {stats['total_duplicates']}")
        print(f"  Fichiers avec doublons: {stats['files_with_duplicates']}")
        
        if stats['total_duplicates'] > 0:
            duplicate_rate = (stats['total_duplicates'] / stats['total_offers']) * 100
            print(f"  Taux de duplication: {duplicate_rate:.2f}%")
            print("\n  ðŸ’¡ Pour nettoyer les doublons, utilisez:")
            print("     python scripts/maintenance/deduplicate_offers.py")
        else:
            print("\n  âœ… Aucun doublon trouvÃ© dans les fichiers !")
        
        print("=" * 80)


if __name__ == "__main__":
    main()
