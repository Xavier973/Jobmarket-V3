# JobMarket â€” Data Job Market Analytics (V3)

## AperÃ§u
JobMarket V3 repart sur une architecture modulaire pour analyser le marche de l'emploi data en France. La collecte d'offres se fait via l'API France Travail, puis les donnees sont normalisees dans un schema commun pour faciliter l'ajout d'autres sources (APEC, Welcome to the Jungle, Indeed, LinkedIn).

## Objectifs
- Remplacer le scraping par l'API France Travail.
- Decoupler l'ingestion, le stockage et la visualisation.
- Garder un schema canonique pour accueillir plusieurs sources.
- Choisir une nouvelle solution de dashboard (etude en cours).

## Architecture cible
- **Ingestion** : pipeline multi-sources (adapters par source) -> donnees brutes -> normalisees.
- **Stockage** : moteur d'analyse (Elasticsearch par defaut) et indexation optimisee.
- **API** : service de lecture pour exposer les donnees au dashboard.
- **Dashboard** : front-end a definir, connecte a l'API.

## Stack technique (base)
- Python 3.9+
- Elasticsearch 8.x (par defaut)
- Docker + Docker Compose

## Structure du projet (V3)
```
Jobmarket_V3/
â”œâ”€â”€ pipelines/           # Pipeline d'ingestion et normalisation
â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”œâ”€â”€ sources/     # Adapters par source (francetravail, apec, etc.)
â”‚   â”‚   â”œâ”€â”€ models.py    # SchÃ©ma canonique JobOffer
â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â””â”€â”€ io.py
â”‚   â””â”€â”€ storage/         # Module de stockage Elasticsearch
â”‚       â”œâ”€â”€ elasticsearch.py
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/             # Scripts utilitaires
â”‚   â”œâ”€â”€ index_to_elasticsearch.py  # Indexation dans Elasticsearch
â”‚   â”œâ”€â”€ analysis/        # Scripts d'analyse des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ analyze_data_analyst.py
â”‚   â”‚   â””â”€â”€ examples_visualization.py
â”‚   â””â”€â”€ maintenance/     # Scripts de maintenance
â”‚       â””â”€â”€ fix_line_endings.py
â”‚
â”œâ”€â”€ tests/               # Tests de validation
â”‚   â””â”€â”€ test_enriched_mapping.py
â”‚
â”œâ”€â”€ data/                # DonnÃ©es brutes et normalisÃ©es
â”‚   â”œâ”€â”€ raw/francetravail/
â”‚   â””â”€â”€ normalized/francetravail/
â”‚
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ data-model.md
â”‚   â”œâ”€â”€ elasticsearch.md
â”‚   â”œâ”€â”€ guide-collecte-francetravail.md
â”‚   â””â”€â”€ ops.md
â”‚
â”œâ”€â”€ config/              # Configuration
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ docker-compose.yml   # Elasticsearch + Kibana
â””â”€â”€ requirements.txt     # DÃ©pendances Python
```

## Configuration
Exemple d'environnement : [config/.env.example](config/.env.example)

Variables principales :
- FT_API_BASE_URL=https://api.francetravail.io/partenaire/offresdemploi
- FT_API_TOKEN_URL=https://entreprise.francetravail.fr/connexion/oauth2/access_token?realm=%2Fpartenaire
- FT_API_CLIENT_ID=
- FT_API_CLIENT_SECRET=
- FT_API_SEARCH_URL=https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search
- FT_API_SCOPE=api_offresdemploiv2 o2dsoffre
- INGEST_OUTPUT_DIR=./data

## DÃ©marrage rapide (ingestion France Travail)
1. Copier l'environnement :
   ```bash
   copy config/.env.example config/.env
   ```
2. Renseigner les variables France Travail.
3. Lancer l'ingestion :
   ```bash
   # Collecte complÃ¨te avec dÃ©coupage automatique (contourne la limite de 150)
   python -m pipelines.ingest.sources.francetravail.main --keywords "data analyst" --split-by-contract
   python -m pipelines.ingest.sources.francetravail.main --keywords "data engineer" --split-by-contract
   
   # Collecte simple (limitÃ©e Ã  150 offres par l'API)
   python -m pipelines.ingest.sources.francetravail.main --keywords "data analyst"
   
   # Collecte limitÃ©e (nombre d'offres spÃ©cifique)
   python -m pipelines.ingest.sources.francetravail.main --keywords "data analyst" --limit 200
   
   # Collecte par codes ROME
   python -m pipelines.ingest.sources.francetravail.main --rome-codes M1419,M1811,M1405
   
   # Mode Ã©chantillon (test rapide)
   python -m pipelines.ingest.sources.francetravail.main --sample
   ```

## DÃ©marrage Elasticsearch et indexation
1. DÃ©marrer Elasticsearch et Kibana :
   ```bash
   # DÃ©marrer les conteneurs Docker
   docker-compose up -d
   
   # VÃ©rifier que les services sont dÃ©marrÃ©s
   docker-compose ps
   ```

2. Installer les dÃ©pendances Python :
   ```bash
   pip install -r requirements.txt
   ```

3. Indexer les donnÃ©es dans Elasticsearch :
   ```bash
   # Indexer toutes les offres France Travail
   python scripts/index_to_elasticsearch.py --source francetravail
   
   # Indexer un fichier spÃ©cifique
   python scripts/index_to_elasticsearch.py --source francetravail --file offers_kw_data_engineer.jsonl
   
   # Forcer la recrÃ©ation de l'index (supprime les donnÃ©es existantes)
   python scripts/index_to_elasticsearch.py --source francetravail --force
   ```

4. AccÃ©der aux interfaces :
   - **Elasticsearch** : http://localhost:9200
   - **Kibana** : http://localhost:5601

ðŸ“– Pour plus de dÃ©tails, voir [docs/elasticsearch.md](docs/elasticsearch.md)

## Analyse des donnÃ©es collectÃ©es
```bash
# Analyser les offres Data Analyst
python scripts/analysis/analyze_data_analyst.py

# Exemples de visualisations (salaires, compÃ©tences, etc.)
python scripts/analysis/examples_visualization.py

# Valider le mapping enrichi
python tests/test_enriched_mapping.py
```

## Maintenance
```bash
# Corriger les fins de ligne des fichiers JSONL
python scripts/maintenance/fix_line_endings.py
```

## Roadmap courte
- Etude comparative du dashboard (voir [docs/dashboard-eval.md](docs/dashboard-eval.md)).
- Mise en place du service API.
- âœ… Indexation ElasticSearch et tests d'aggregations.
- Ajout d'une 2eme source (APEC ou WTTJ) pour valider l'extensibilite.

##  Troubleshooting API France Travail
- Erreur 401: verifier `FT_API_CLIENT_ID`, `FT_API_CLIENT_SECRET` et `FT_API_SCOPE`.
- Erreur 400: verifier `FT_API_TOKEN_URL` et le format `application/x-www-form-urlencoded`.
- Erreur 429: respecter `Retry-After` et limiter le nombre d'appels par seconde.
- Aucun resultat: verifier les parametres de recherche et tester en mode bac a sable.
- **Pagination avec le paramÃ¨tre `range`**: L'API utilise le paramÃ¨tre `range` (format `"0-149"`, `"150-299"`) au lieu de `page`/`size`. Maximum 1150 offres par recherche (range 0-149 + 150-299 + ... + 1000-1149). Pour aller au-delÃ , utiliser `--split-by-contract` ou subdiviser par dates (`minCreationDate`/`maxCreationDate`).
- **Ã‰cart avec le site web**: Le site France Travail affiche beaucoup plus d'offres (ex: 1337 pour "data engineer" vs 353 via l'API). **Raison principale** : le site utilise une recherche floue qui renvoie des rÃ©sultats peu pertinents (ex: "DÃ©veloppeur COBOL" apparaÃ®t pour "data engineer"). L'API est plus stricte et ne renvoie que les offres vraiment pertinentes. **Conclusion** : Les 353 offres API sont de meilleure qualitÃ© que les 1337 du site (moins de bruit). Le site inclut aussi quelques offres partenaires (Indeed, Monster, LinkedIn) non accessibles via l'API publique.

## Licence
Projet interne / usage prive.